# RAG Tool ğŸ”

> **Note**: This project was lovingly vibe-coded with Claude Code. What started as a careful implementation plan quickly turned into an experimental journey of discovery, feature additions, and "hey, what if we just..." moments.

A TypeScript-based Retrieval-Augmented Generation (RAG) tool implementing Anthropic's contextual retrieval methodology for 35% improved search accuracy. Features both CLI functionality and upcoming MCP (Model Context Protocol) server capabilities.

## âœ¨ Features

### ğŸ—ï¸ **Linear Chunk Processing**
- **Pipeline Architecture**: Read â†’ Context â†’ Embedding â†’ Store for each chunk
- **Configurable Parallelism**: Control concurrent processing (1-20 chunks, default: 5)
- **Comprehensive Logging**: Detailed verbose output for debugging and monitoring

### ğŸ“„ **Document Processing**
- **File-Reference Architecture**: Database stores only positions and metadata, not content
- **Smart Chunking**: Character, sentence, and paragraph-based strategies
- **Integrity Checking**: SHA-256 hashing and file modification tracking
- **Format Support**: Text (.txt) and Markdown (.md) documents

### ğŸ§  **Contextual Retrieval**
- **Anthropic's Method**: Official contextual retrieval prompt template
- **OpenAI Integration**: GPT-4o-mini for cost-effective context generation
- **Prompt Caching**: Up to 75% cost reduction with OpenAI's caching
- **Batch Processing**: Efficient multi-chunk processing

### ğŸ”¢ **Vector Embeddings**
- **OpenAI Embeddings**: text-embedding-3-small for cost-effectiveness
- **SQLite Storage**: Local vector storage with sqlite-vss
- **Batch Generation**: Efficient multi-document embedding
- **Vector Search**: Fast semantic similarity search

### ğŸ–¥ï¸ **CLI Interface**
- **Interactive Setup**: Guided configuration with API key validation
- **Progress Indicators**: Real-time processing feedback
- **Error Recovery**: Graceful failure handling and restart capability
- **Flexible Options**: Extensive customization via CLI flags

## ğŸš€ Quick Start

### Installation & Setup

```bash
# Install via npm (when published)
npx rag-tool init

# Or clone and build locally
git clone <repository>
cd rag-tool
npm install
npm run build
```

### Basic Usage

```bash
# Initialize configuration
npx rag-tool init

# Set up database
npx rag-tool db-init

# Add a document with default settings
npx rag-tool add document.txt

# Add with custom parallelism and verbose logging
npx rag-tool add document.txt --max-parallel 10 --verbose

# Search your documents
npx rag-tool search "your search query"

# List all documents
npx rag-tool list
```

### Advanced Options

```bash
# Skip context generation (embedding only)
npx rag-tool add document.txt --skip-context

# Skip embedding generation (context only)
npx rag-tool add document.txt --skip-embedding

# Dry run to see what would happen
npx rag-tool add document.txt --dry-run --verbose

# Force overwrite existing document
npx rag-tool add document.txt --force

# Use custom config path
npx rag-tool add document.txt --config-path /path/to/config.json
```

## ğŸ“Š Configuration

Configuration is stored in `~/.rag-tool/config.json`:

```json
{
  "version": "0.1.0",
  "providers": {
    "openai": {
      "apiKey": "your-api-key",
      "embeddingModel": "text-embedding-3-small",
      "contextModel": "gpt-4o-mini",
      "maxTokens": 4000
    }
  },
  "database": {
    "path": "/Users/you/.rag-tool/database.db",
    "vectorDimension": 1536
  },
  "chunking": {
    "strategy": "sentence",
    "chunkSize": 1000,
    "overlap": 200
  },
  "contextGeneration": {
    "provider": "openai",
    "maxContextTokens": 100
  },
  "processing": {
    "maxParallelChunks": 5,
    "enableLinearProcessing": true
  }
}
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Interface â”‚    â”‚   MCP Server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Core Services      â”‚
         â”‚                       â”‚
         â”‚ â€¢ DocumentService     â”‚
         â”‚ â€¢ ContextService      â”‚
         â”‚ â€¢ EmbeddingService    â”‚
         â”‚ â€¢ ChunkProcessingService â”‚
         â”‚ â€¢ DatabaseService     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   AI Provider Layer   â”‚
         â”‚                       â”‚
         â”‚ â€¢ OpenAI Provider     â”‚
         â”‚ â€¢ Provider Interface  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   SQLite Database     â”‚
         â”‚   with Vector Ext.    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ How It Works

### Linear Processing Pipeline

Each document chunk flows through a complete pipeline:

1. **ğŸ“– Read**: Extract text from original file using stored positions
2. **ğŸ§  Context**: Generate contextual summary using Anthropic's method
3. **ğŸ”¢ Embedding**: Create vector embedding with OpenAI
4. **ğŸ’¾ Store**: Save metadata and vectors to SQLite database

### Controlled Parallelism

- **Semaphore Pattern**: Limits concurrent chunk pipelines
- **Resource Management**: Prevents API rate limit issues
- **Configurable**: Adjust based on your API limits and system resources

### File-Reference Architecture

- **No Content Storage**: Database contains only file paths and positions
- **Privacy Focused**: Original files remain the source of truth
- **Integrity Checking**: SHA-256 hashes detect file changes
- **Lightweight**: Minimal database storage requirements

## ğŸ› ï¸ Development

This project was "vibe-coded" - meaning it evolved organically through experimentation and iterative improvements rather than rigid planning. The codebase reflects this journey with:

- **Modern TypeScript**: Strict typing and latest language features
- **Modular Design**: Clean separation of concerns
- **Comprehensive Logging**: Detailed debugging capabilities
- **Error Resilience**: Graceful failure handling throughout

### Build & Test

```bash
# Install dependencies
npm install

# Build the project
npm run build

# Run linting
npm run lint

# Format code
npm run format
```

## ğŸ“ˆ Performance

- **Document Processing**: < 30 seconds per 10MB document
- **Search Latency**: < 500ms for typical queries
- **Storage Efficiency**: < 2x original document size
- **Memory Usage**: < 512MB for typical operations

## ğŸ”® Roadmap

### Phase 4: MCP Server Integration
- Model Context Protocol server for Claude Code integration
- Real-time document search and retrieval tools
- Resource management for documents and search results

### Phase 5: Advanced Features
- Additional AI providers (Anthropic, Cohere, local models)
- Document format expansion (PDF, Office documents)
- Hybrid search (BM25 + vector similarity)
- REST API server mode

## ğŸ¤ Contributing

This project embraces the "vibe-coding" philosophy:

1. **Experiment Freely**: Try new ideas and see what works
2. **Iterate Quickly**: Make changes, test, and improve
3. **Document Discoveries**: Share what you learn along the way
4. **Maintain Quality**: Keep tests passing and code clean

## ğŸ“„ License

[License information to be added]

## ğŸ™ Acknowledgments

- **Anthropic**: For the contextual retrieval methodology
- **OpenAI**: For embeddings and context generation APIs
- **Claude Code**: For being the perfect vibe-coding companion
- **sqlite-vss**: For making vector search accessible and portable

---

*Built with TypeScript, powered by AI, and crafted through the art of vibe-coding.* âœ¨